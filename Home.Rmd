---
title: "STATS 506 Project: Logistic Regression with Model Diagnostics"
author: "Group 5: Fang, Marwah, and Santos"
date: "11/27/2018"
output: 
   html_document:
     toc: false
---
The goal of this tutorial is to demostrate the use of Logistic Regression, and model diagnostics for this type of regression. We will start this tutorial by explaining the algorithm and the modeling behind Logistic regression. This would be followed by an illustrative example using three statistical software languages: Python, R, and STATA.

##Algorithm
Logistic regression is a predictive modelling algorithm that is used when the response variable is binary categorical variable, i.e. it can only take two values: 0 and 1. 

##Data Summary

In this tutorial, we work on the `pima` dataset from `faraway v1.0.7` in R, 
which has 9 variables. The data comes from a National Institute of Diabetes and 
Digestive and Kidney Diseases study on 768 adult female Pima Indians living near Phoenix.
Different measurements of the subjects were taken and it was recorded whether or not
the patient showed signs of diabetes.

##Languages {.tabset}

###R

In order to run this analysis in R, we need to install two key packages: "Faraway" and "ResourceSelection" The former is required to access the data used in this illustration and the latter is used to run the model diagnostics. Once installed with the command, install.packages(“packagename”), they can be loaded using library( ).

**Loading Dataframe and Numerical Summaries**

Once the dataframe "pima" is loaded, we can view the summaries of the variables by using the function, summary.

```{r pima}
library(faraway)
data(pima)
summary(pima)
```

The variables: "diastolic", "insulin", "glucose" and "bmi" exhibit zero values, since that is not physically possible we conclude that these are measurement errors or missing values might have been coded as 0. In an attempt to clean up the data, we remove these values.

```{r message = FALSE}
library(dplyr)
pima_clean = filter(pima, diastolic > 0 & bmi > 0) %>% filter(insulin > 0 & glucose > 0)
summary(pima_clean)
```

**Model Selection using the Backward Elimination method**

We use the Backward elimination method to select the model specification, with the BIC as a guide. Please note that a lower BIC indicates improved fit of the model. So, we will be focusing on the individual p-values for the variables and the BIC for the model. 

We start with a logistic model with "test" as the response variable and all the other variables as explanatory. 
```{r logisticmodel}
Fit = glm(test~ ., family = binomial(link="logit"), data = pima_clean)
summary(Fit)
BIC(Fit)
```
The stepwise elimination of variables that are individually statistically insignificant and improve the BIC, results in the final model specification with four idependent variables: age, bmi, glucose, and diabetes.
```{r finalmodel}
Fit_final = glm(test~ glucose+bmi+diabetes+age, family = binomial(link="logit"), data = pima_clean)
summary(Fit_final)
BIC(Fit_final)
```
The estimated coefficients indicate the change in the log odds of the response when the predictors change by one unit. For example, the log odds of testing positive for diabetes increases by 0.053 when age increases by one unit.In addition, we can obtain the confidence intervals for the coefficient estimates as given below (Note, that these confidence intervals are based on the standard errors and rely on the normality assumption).
```{r message = FALSE}
#computing confidence intervals:
confint(Fit_final)
```

**Goodness of Fit**

*Hosmer-Lemeshow Test*

We apply the Hosmer-Lemeshow test to assess the fit of the model. We check the null hypothesis that the specified logistic regression model is the correct model. Since the p-value is high, we fail to reject the null hypothesis.
```{r message = FALSE}
library(ResourceSelection)
h1 = hoslem.test(Fit_final$y, fitted(Fit_final), g = 10)
h1
```

In addition, the Hosmer–Lemeshow test specifically identifies subgroups as the deciles of fitted probability values. Models for which the expected and observed probabilities in subgroups are similar are considered to be well calibrated. This provides evidence that our model is well calibrated.

```{r HL1}
library(ResourceSelection)
#inspect the expected and observed values
h2 = cbind(h1$expected, h1$observed)
h2
```

**Confusion Matrix**

This provides us with another tool to evaluate the model. It gives us a measure of the Type 1 and Type 2 error in our modeling. Thus the diagonal gives the count of the instances when our model predicts correctly. Inspecting the Confusion matrix for our data we deduce that the overall predicted accurancy for our model is around 80 percent. 
```{r CM}
p = predict(Fit_final, pima_clean, type = "response")
Con_table = table(p > 0.5, pima_clean$test)
Con_table
```

**Reciever Operating Characteristic (ROC) Curve**

Finally, we inspect the ROC curve. This curve plots the the false positive rate against the true probablistic prediction for a range of threshold probabilities. The area under the curve is viewed as a measure of prediction accuracy. The larger the area under the curve, and hence the farther away the ROC curve is from the diagonal, the better the model performance. Computing the area under the curve (AUC) allows us to quantitatively evaluate the model, performance. This could serve as a useful tool for model comparision as well.


```{r roc, message = FALSE}
library(ROCR)
p = predict(Fit_final, pima_clean, type = "response")
pred = prediction(p, pima_clean$test)
roc = performance(pred, "tpr", "fpr")
plot(roc,
     main = "ROC Curve")
abline(a=0, b=1)
#Higher area under the curve the better the fit (AUC)
auc = performance(pred, "auc")
auc = unlist(slot(auc, "y.values"))
auc = round(auc, 2)
legend(.6, .2, auc, title = "Area under the Curve", cex = .75)
```


**Summary**

In this tutorial we illustrated how to implement logistic regression and conduct model diagnostics. 

**References**

Hosmer, D. & Lemeshow, S. (2000). Applied Logistic Regression (Second Edition). New York: John Wiley & Sons, Inc.
UCLA Institute for Digital Research and Education website: https://stats.idre.ucla.edu/r/dae/logit-regression/


###Python 



###STATA
